{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RishabhJha395/ytsentiment_analysis/blob/main/Copy_of_youtubeCommentSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_3CQlhq1FVP"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk, re, os\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Nb5eZU1K21"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get(\"YOUTUBE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngn9kN-E5Jfc"
      },
      "outputs": [],
      "source": [
        "def extract_video_id(url):\n",
        "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid YouTube video URL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46r93FBn5Mqx"
      },
      "outputs": [],
      "source": [
        "def fetch_video_details(video_id):\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    if not response[\"items\"]:\n",
        "        return None\n",
        "\n",
        "    data = response[\"items\"][0]\n",
        "    snippet = data[\"snippet\"]\n",
        "    stats = data[\"statistics\"]\n",
        "\n",
        "    # Fetch category name (requires extra API call)\n",
        "    category_id = snippet[\"categoryId\"]\n",
        "    cat_request = youtube.videoCategories().list(\n",
        "        part=\"snippet\", id=category_id\n",
        "    )\n",
        "    cat_response = cat_request.execute()\n",
        "    category_name = cat_response[\"items\"][0][\"snippet\"][\"title\"]\n",
        "\n",
        "    video_data = {\n",
        "        \"title\": snippet[\"title\"],\n",
        "        \"channel\": snippet[\"channelTitle\"],\n",
        "        \"published_date\": snippet[\"publishedAt\"],\n",
        "        \"category\": category_name,\n",
        "        \"views\": int(stats.get(\"viewCount\", 0)),\n",
        "        \"likes\": int(stats.get(\"likeCount\", 0)) if \"likeCount\" in stats else 0,\n",
        "        \"comments\": int(stats.get(\"commentCount\", 0)) if \"commentCount\" in stats else 0,\n",
        "    }\n",
        "    return video_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u59qXQVV5PlG"
      },
      "outputs": [],
      "source": [
        "def process_youtube_video(video_url):\n",
        "    # --- Extract video info ---\n",
        "    video_id = extract_video_id(video_url)\n",
        "    video_info = fetch_video_details(video_id)\n",
        "\n",
        "    print(\"üé• Video Details:\")\n",
        "    for k, v in video_info.items():\n",
        "        print(f\"{k.title()}: {v}\")\n",
        "    print()\n",
        "\n",
        "    # --- Fetch comments ---\n",
        "    comments = fetch_comments(video_id)\n",
        "    df = pd.DataFrame(comments, columns=[\"Comment\"])\n",
        "\n",
        "    # --- Clean and Analyze ---\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer\n",
        "    import re\n",
        "    from textblob import TextBlob\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ps = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Clean comment text\n",
        "    def clean_text(text):\n",
        "        text = re.sub(r\"http\\S+\", \"\", text)         # remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z]\", \" \", text)      # keep only letters\n",
        "        text = text.lower()\n",
        "        words = text.split()\n",
        "        words = [ps.stem(w) for w in words if w not in stop_words]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    # Analyze sentiment using TextBlob\n",
        "    def analyze_sentiment(text):\n",
        "        if not text.strip():\n",
        "            return \"Neutral\"\n",
        "        polarity = TextBlob(text).sentiment.polarity\n",
        "        if polarity > 0.1:\n",
        "            return \"Positive\"\n",
        "        elif polarity < -0.1:\n",
        "            return \"Negative\"\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "\n",
        "    df[\"Cleaned\"] = df[\"Comment\"].apply(clean_text)\n",
        "    df[\"Sentiment\"] = df[\"Cleaned\"].apply(analyze_sentiment)\n",
        "\n",
        "    # --- Summary ---\n",
        "    summary = df[\"Sentiment\"].value_counts().to_dict()\n",
        "    total = len(df)\n",
        "    for k in summary:\n",
        "        summary[k] = round(summary[k] / total * 100, 2)\n",
        "\n",
        "    print(\"\\nüìä Sentiment Distribution (in %):\")\n",
        "    for k, v in summary.items():\n",
        "        print(f\"{k}: {v}%\")\n",
        "\n",
        "    # --- Save results ---\n",
        "    df.to_csv(\"youtube_comments_with_sentiment.csv\", index=False)\n",
        "    print(\"\\nüíæ Saved comments with sentiments to CSV\")\n",
        "\n",
        "    # --- Visualization ---\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.pie(summary.values(), labels=summary.keys(), autopct='%1.1f%%', startangle=90)\n",
        "    plt.title(\"YouTube Comment Sentiment Pie Chart\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- Like Ratio ---\n",
        "    like_ratio = 0\n",
        "    if video_info[\"views\"] > 0:\n",
        "        like_ratio = round(video_info[\"likes\"] / video_info[\"views\"] * 100, 2)\n",
        "\n",
        "    print(f\"\\nüëç Like Ratio: {like_ratio}% of viewers liked this video\")\n",
        "\n",
        "    # Return data for further use\n",
        "    return df, summary, video_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp5rtvjN-QlQ"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "    comments = []\n",
        "\n",
        "    # YouTube comments API\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,  # max 100 per page\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "\n",
        "    while request and len(comments) < max_results:\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response[\"items\"]:\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "\n",
        "        # Handle pagination\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "\n",
        "        if len(comments) >= max_results:\n",
        "            break\n",
        "\n",
        "    return comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE6KjyoxRiRn"
      },
      "source": [
        "##Sentimental Analysis along with summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr00grnr5UBT"
      },
      "outputs": [],
      "source": [
        "video_url = input(\"Enter YouTube video URL: \")\n",
        "df, summary, video_info = process_youtube_video(video_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn1fh_BVReP7"
      },
      "source": [
        "##Most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv50B4IO5toa"
      },
      "outputs": [],
      "source": [
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "positive_comments = \" \".join(df[df['Sentiment']=='Positive']['Cleaned'])\n",
        "negative_comments = \" \".join(df[df['Sentiment']=='Negative']['Cleaned'])\n",
        "\n",
        "wc_pos = WordCloud(width=800, height=400, background_color='white', colormap='Green').generate(positive_comments)\n",
        "wc_neg = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(negative_comments)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wc_pos)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud - Positive Comments\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wc_neg)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud - Negative Comments\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwI9_hMNRZnI"
      },
      "source": [
        "##Sentiment Polarity Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd-5ZZFiN6_9"
      },
      "outputs": [],
      "source": [
        "df[\"Polarity\"] = df[\"Cleaned\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(df[\"Polarity\"], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Sentiment Polarity Distribution\")\n",
        "plt.xlabel(\"Polarity (-1 = Negative, +1 = Positive)\")\n",
        "plt.ylabel(\"Comment Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xn7Oaw-RTHr"
      },
      "source": [
        "##Emotion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st-2BdnpOCYI"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the emotion classification model\n",
        "emotion_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\"\n",
        ")\n",
        "\n",
        "sample = input(\"Enter the Comment for which you want the emotion     :\")\n",
        "result = emotion_classifier(sample)[0]   # get first (and only) prediction\n",
        "print(result['label'].upper())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6QFzxLCRMUI"
      },
      "source": [
        "###Keyword Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygEh7EPIRRUC"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_words = \" \".join(df[\"Cleaned\"]).split()\n",
        "common_words = Counter(all_words).most_common(10)\n",
        "print(common_words)\n",
        "\n",
        "\n",
        "words, counts = zip(*common_words)\n",
        "plt.bar(words, counts)\n",
        "plt.title(\"Top 10 Common Words in Comments\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNsw7QTjRxjg"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summary_text = summarizer(\" \".join(df[\"Comment\"][:50]), max_length=80, min_length=25, do_sample=False)\n",
        "print(summary_text[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zccb6I-SAFF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO9qj8Xrp932sGNnm3Fjn6d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
